{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to Random Forests.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atlas-github/Lots_of_Useful_Code/blob/master/Introduction_to_Random_Forests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG1g1VztqCHn",
        "colab_type": "text"
      },
      "source": [
        "Source: [Introduction to Random Forests](https://www.datascience.com/resources/notebooks/random-forest-intro)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_0-wHzOqMMQ",
        "colab_type": "text"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-6NjF-_qzXW",
        "colab_type": "text"
      },
      "source": [
        "This piece was adapted with permission from the author from [inertia7.com](https://www.inertia7.com/). View the original [here](https://www.inertia7.com/projects/95). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2NmUaQSq72n",
        "colab_type": "text"
      },
      "source": [
        "Random forests, also known as random decision forests, are a popular ensemble method that can be used to build predictive models for both classification and regression problems. Ensemble methods use multiple learning models to gain better predictive results — in the case of a random forest, the model creates an entire forest of random uncorrelated decision trees to arrive at the best possible answer.\n",
        "\n",
        "To demonstrate how this works in practice — specifically in a classification context — I’ll be walking you through an example using a famous data set from the University of California, Irvine (UCI) Machine Learning Repository. The data set, called the Breast Cancer Wisconsin (Diagnostic) Data Set, deals with binary classification and includes features computed from digitized images of biopsies. The data set can be downloaded [here](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29).\n",
        "\n",
        "To follow this tutorial, you will need some familiarity with classification and regression tree (CART) modeling. I will provide a brief overview of different CART methodologies that are relevant to random forest, beginning with decision trees. If you’d like to brush up on your knowledge of CART modeling before beginning the tutorial, I highly recommend reading Chapter 8 of the book “An Introduction to Statistical Learning with Applications in R,” which can be downloaded [here](http://www-bcf.usc.edu/~gareth/ISL/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omMLNyzDrLsY",
        "colab_type": "text"
      },
      "source": [
        "# Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95_hCmwirNcV",
        "colab_type": "text"
      },
      "source": [
        "Decision trees are simple but intuitive models that utilize a top-down approach in which the root node creates binary splits until a certain criteria is met. This binary splitting of nodes provides a predicted value based on the interior nodes leading to the terminal (final) nodes. In a classification context, a decision tree will output a predicted target class for each terminal node produced.\n",
        "\n",
        "Although intuitive, decision trees have limitations that prevent them from being useful in [machine learning applications](https://www.datascience.com/blog/machine-learning-and-deep-learning-what-is-the-difference). You can learn more about implementing a decision tree [here](http://scikit-learn.org/stable/modules/tree.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLpFlXuTrYQ_",
        "colab_type": "text"
      },
      "source": [
        "## Limitations to Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oORsKyfradJ",
        "colab_type": "text"
      },
      "source": [
        "Decision trees tend to have high variance when they utilize different training and test sets of the same data, since they tend to overfit on training data. This leads to poor performance on unseen data. Unfortunately, this limits the usage of decision trees in predictive modeling. However, using ensemble methods, we can create models that utilize underlying decision trees as a foundation for producing powerful results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWKes2K9rdUp",
        "colab_type": "text"
      },
      "source": [
        "# Bootstrap Aggregating Trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gr6k8Grpwpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}